# Dataset Forge - Comprehensive Development Standards

You are an expert Python developer and AI assistant working on the Dataset Forge project - a comprehensive Python CLI utility for managing, analyzing, and transforming image datasets, especially High-Quality (HQ) and Low-Quality (LQ) pairs for super-resolution and ML tasks.

## Core Development Principles

### Technology Stack
- **Python Version**: Python 3.8+ (Dataset Forge requirement)
- **Virtual Environment**: Always use `venv312\Scripts\activate`
- **Code Formatting**: Black formatter (88 character line length)
- **Type Annotations**: Complete type hints using `typing` module
- **Testing Framework**: pytest only (never unittest)
- **Documentation**: Google-style docstrings
- **Dependencies**: pip with requirements.txt

### Core Libraries
- **Image Processing**: PIL/Pillow, OpenCV, imageio, imagehash
- **Machine Learning**: PyTorch, torchvision, transformers, timm, lpips
- **Data Processing**: pandas, numpy, dask, ray
- **Parallel Processing**: joblib, concurrent.futures, multiprocessing
- **Audio Feedback**: pygame, playsound
- **UI/CLI**: questionary, tqdm for progress tracking

## Catppuccin Mocha Color Scheme - Project-Wide Rule

- **MANDATORY:** All user-facing CLI output, prompts, progress bars, and menus **must** use the Catppuccin Mocha color scheme, via the centralized color utilities (`from dataset_forge.utils.color import Mocha`).
- **All printing must use the centralized printing utilities** (`print_info`, `print_success`, `print_warning`, `print_error`, `print_header`, `print_section`, etc.) with Mocha colors.
- **No raw print statements** are allowed in user-facing code.
- **All new features, menus, and workflows must be reviewed for consistent color usage before merging.**
- **Tests for CLI output should check for colorized output where possible.**
- **Documentation and code examples must demonstrate the use of Mocha colors and centralized printing.**

## Architecture Patterns

### Modular Design
- **menus/**: UI components and user interactions
- **actions/**: Business logic and core functionality
- **utils/**: Shared utilities and helper functions
- **dpid/**: Degradation Process Implementation Details

### Centralized Utilities
- **Memory Management**: `dataset_forge.utils.memory_utils`
- **Parallel Processing**: `dataset_forge.utils.progress_utils`
- **Error Handling**: `dataset_forge.utils.printing`
- **Audio Feedback**: `dataset_forge.utils.audio_utils`
- **Session State**: User preferences and configuration

## Code Style Standards

### Naming Conventions
- **Variables**: snake_case
- **Functions**: snake_case
- **Classes**: PascalCase
- **Constants**: UPPER_SNAKE_CASE
- **Files**: snake_case.py
- **Directories**: snake_case/

### Import Organization
```python
# 1. Standard library imports
import os
import sys
from typing import List, Optional, Dict, Any, Union, Tuple
from pathlib import Path

# 2. Third-party imports
import numpy as np
import torch
from PIL import Image

# 3. Local imports (dataset_forge.*)
from dataset_forge.utils.memory_utils import clear_memory, clear_cuda_cache
from dataset_forge.utils.printing import print_info, print_success, print_warning, print_error
from dataset_forge.utils.progress_utils import smart_map, image_map
from dataset_forge.utils.parallel_utils import ProcessingType, ParallelConfig

# 4. Relative imports (if within same module)
from .utils import helper_function
```

## Memory Management

### Centralized Memory Management
```python
from dataset_forge.utils.memory_utils import (
    clear_memory,
    clear_cuda_cache,
    memory_context,
    auto_cleanup,
    to_device_safe,
    safe_cuda_operation,
    get_memory_manager,
)

# Basic memory cleanup
def process_images(images):
    """Process images with automatic memory cleanup."""
    try:
        results = []
        for image in images:
            result = process_single_image(image)
            results.append(result)
        return results
    finally:
        clear_memory()
        clear_cuda_cache()

# Context manager pattern
def process_large_dataset(images):
    """Process large dataset with memory context."""
    with memory_context("Large Dataset Processing", cleanup_on_exit=True):
        for batch in chunk_images(images, batch_size=32):
            process_batch(batch)
            clear_memory()  # Clear after each batch

# Automatic cleanup decorator
@auto_cleanup
def process_images_with_decorator(images):
    """Process images with automatic cleanup decorator."""
    results = []
    for image in images:
        result = process_single_image(image)
        results.append(result)
    return results

# CUDA memory management
def gpu_operation(tensor):
    """Safe GPU operation with memory management."""
    try:
        tensor = to_device_safe(tensor, "cuda")
        result = perform_gpu_operation(tensor)
        return result
    finally:
        clear_cuda_cache()
```

## Parallel Processing

### Centralized Parallel Processing
```python
from dataset_forge.utils.progress_utils import (
    smart_map,
    image_map,
    process_map,
    thread_map,
)
from dataset_forge.utils.parallel_utils import (
    ParallelConfig,
    ProcessingType,
    parallel_map,
    parallel_image_processing,
)

# Smart map pattern
def process_items_with_smart_map(items):
    """Process items with automatic optimization."""
    results = smart_map(
        process_single_item,
        items,
        desc="Processing items",
        max_workers=4,
        processing_type=ProcessingType.AUTO,
    )
    return results

# Image map pattern
def process_images_with_image_map(image_paths):
    """Process images with specialized optimization."""
    results = image_map(
        process_single_image,
        image_paths,
        desc="Processing images",
        max_workers=4,
    )
    return results

# Processing type selection
def setup_parallel_processing():
    """Setup parallel processing configuration."""
    config = ParallelConfig(
        max_workers=4,
        processing_type=ProcessingType.THREAD,  # I/O bound task
        use_gpu=True,
        gpu_memory_fraction=0.8,
        chunk_size=1,
    )
    return config
```

## Error Handling

### Centralized Error Handling
```python
from dataset_forge.utils.printing import (
    print_error,
    print_warning,
    print_info,
    print_success,
)
from dataset_forge.utils.history_log import log_operation
from dataset_forge.utils.audio_utils import play_error_sound

def process_images_with_error_handling(image_paths: List[str]) -> List[str]:
    """Process images with comprehensive error handling."""
    results = []
    
    for image_path in image_paths:
        try:
            result = process_single_image(image_path)
            results.append(result)
            
        except FileNotFoundError as e:
            print_error(f"Image file not found: {image_path}")
            log_operation("process_images", f"Failed: File not found - {image_path}")
            continue
            
        except PermissionError as e:
            print_error(f"Permission denied accessing: {image_path}")
            log_operation("process_images", f"Failed: Permission denied - {image_path}")
            continue
            
        except Exception as e:
            print_error(f"Unexpected error processing {image_path}: {e}")
            log_operation("process_images", f"Failed: Unexpected error - {image_path} - {e}")
            continue
    
    return results

def robust_operation():
    """Robust operation with comprehensive error handling."""
    try:
        result = perform_operation()
        log_operation("operation_name", "Operation completed successfully")
        return result
        
    except FileNotFoundError as e:
        print_error(f"File not found: {e}")
        log_operation("operation_name", f"Failed: File not found - {e}")
        return None
        
    except MemoryError as e:
        print_error(f"Memory error: {e}")
        log_operation("operation_name", f"Failed: Memory error - {e}")
        clear_memory()
        clear_cuda_cache()
        return None
        
    except Exception as e:
        print_error(f"Unexpected error: {e}")
        log_operation("operation_name", f"Failed: Unexpected error - {e}")
        return None
```

## Documentation Standards

### Google-Style Docstrings
```python
def process_images(image_paths: List[str], output_dir: str) -> List[str]:
    """Process a list of images and save results to output directory.
    
    This function processes multiple images in parallel, applying transformations
    and saving the results to the specified output directory. It includes
    comprehensive error handling and memory management.
    
    Args:
        image_paths: List of input image file paths to process
        output_dir: Directory path where processed images will be saved
        
    Returns:
        List of successfully processed image file paths
        
    Raises:
        FileNotFoundError: If any input image file doesn't exist
        PermissionError: If output directory is not writable
        ValueError: If any image file is corrupted or unsupported format
        MemoryError: If insufficient memory for processing
        
    Example:
        >>> paths = ['img1.jpg', 'img2.png']
        >>> results = process_images(paths, 'output/')
        >>> print(f"Processed {len(results)} images")
        
    Note:
        This function uses parallel processing for efficiency and includes
        automatic memory cleanup after processing.
    """
    # Implementation here
    pass

class ImageProcessor:
    """A comprehensive image processing class with GPU acceleration support.
    
    This class provides high-level image processing capabilities including
    transformations, filtering, and batch operations. It automatically
    manages GPU memory and provides fallback to CPU when needed.
    
    Attributes:
        device: The processing device ('cuda' or 'cpu')
        batch_size: Number of images to process in each batch
        memory_limit: Maximum memory usage in GB
        
    Example:
        >>> processor = ImageProcessor(device='cuda', batch_size=32)
        >>> results = processor.process_batch(image_paths)
    """
    
    def __init__(self, device: str = 'auto', batch_size: int = 32):
        """Initialize the image processor.
        
        Args:
            device: Processing device ('cuda', 'cpu', 'auto')
            batch_size: Number of images to process in each batch
            
        Raises:
            ValueError: If device is not supported
            RuntimeError: If CUDA is requested but not available
        """
        # Implementation here
        pass
```

## Testing Standards

### CLI Testability
- All help system methods must accept a `pause: bool = True` argument
- Tests must call help/quit with `pause=False` to avoid manual input
- No test should require user input for help/quit/exit scenarios
- Use `pytest.raises(SystemExit)` to test quit/exit commands

### Test Framework Requirements
- **ONLY** use pytest or pytest plugins, **NEVER** use unittest
- **ALWAYS** place tests in `./tests/` directory with proper structure
- **ALWAYS** create necessary `__init__.py` files if they don't exist
- **ALWAYS** include typing annotations in tests
- **ALWAYS** include docstrings in tests

### Test Organization
- **Unit Tests**: `tests/test_utils/` for utility functions and core logic
- **Integration Tests**: `tests/test_cli/` for end-to-end workflows and menu interactions
- **Test Naming**: Use `test_*.py` files with descriptive names
- **Test Functions**: Use descriptive function names starting with `test_`

### Public API Requirements
```python
# All features must provide public, non-interactive APIs for testing
def process_images(image_paths: List[str], output_dir: str) -> List[str]:
    """Process a list of images and save results to output directory.
    
    Args:
        image_paths: List of input image file paths
        output_dir: Directory to save processed images
    Returns:
        List of output image file paths
    Raises:
        FileNotFoundError: If input files don't exist
        PermissionError: If output directory is not writable
    """
    # Implementation here
    pass
```

### Monkeypatching and Dummy Objects
```python
import pytest
from unittest.mock import patch, MagicMock, Mock

def test_feature_with_mock():
    """Test feature using monkeypatching to isolate logic."""
    with patch('dataset_forge.utils.audio_utils.play_done_sound') as mock_audio:
        with patch('dataset_forge.utils.memory_utils.clear_memory') as mock_memory:
            # Test implementation
            result = process_images(['test.jpg'], 'output/')
            # Verify calls
            mock_audio.assert_called_once()
            mock_memory.assert_called_once()
            assert result == ['output/test.jpg']

def test_error_handling():
    """Test error handling and recovery."""
    with pytest.raises(FileNotFoundError):
        process_images(['nonexistent.jpg'], 'output/')
    with pytest.raises(PermissionError):
        process_images(['test.jpg'], '/readonly/')
```

## Menu System Pattern

### Standardized Key-Based Menu Pattern (MANDATORY)
- **ALL menus must use the standardized key-based pattern** to ensure consistent behavior and prevent `'str' object is not callable` errors.
- **show_menu always returns the key** (string/int), never the function directly.
- **All menu loops must follow this exact pattern**:

```python
def my_menu():
    """Menu implementation with standardized key-based pattern."""
    options = {
        "1": ("Option 1", function1),
        "2": ("Option 2", function2),
        "0": ("ðŸšª Exit", None),
    }
    
    while True:
        try:
            key = show_menu("Menu Title", options, Mocha.lavender)
            if key is None or key == "0":
                return
            action = options[key][1]
            if callable(action):
                action()
        except (KeyboardInterrupt, EOFError):
            print_info("\nExiting...")
            break
```

### Menu Implementation Requirements
- **NEVER** use `action = show_menu(...); action()` pattern
- **ALWAYS** use key lookup: `action = options[key][1]`
- **ALWAYS** check if action is callable before executing
- **ALWAYS** handle None and "0" exit cases
- **ALWAYS** use centralized printing utilities with Mocha colors

### Examples

#### âœ… Correct Implementation
```python
def dataset_management_menu():
    """Dataset management menu with standardized pattern."""
    options = {
        "1": ("ðŸ“‚ Create Dataset", create_dataset_action),
        "2": ("ðŸ” Analyze Dataset", analyze_dataset_action),
        "0": ("â¬…ï¸ Back", None),
    }
    
    while True:
        key = show_menu("Dataset Management", options, Mocha.lavender)
        if key is None or key == "0":
            return
        action = options[key][1]
        if callable(action):
            action()
```

#### âŒ Incorrect Implementation (Forbidden)
```python
def bad_menu():
    """This pattern is forbidden and will cause errors."""
    options = {
        "1": ("Option 1", function1),
        "2": ("Option 2", function2),
    }
    
    # âŒ WRONG: This will cause 'str' object is not callable
    action = show_menu("Menu", options, Mocha.lavender)
    action()  # Error: trying to call a string
```

### Error Prevention
- **`'str' object is not callable`** - Fixed by using key lookup
- **Inconsistent menu behavior** - Fixed by standardized pattern
- **Missing exit handling** - Fixed by explicit None/"0" checks
- **Unsafe function calls** - Fixed by callable checks

### Migration Guide
If converting old menus:
1. **Change**: `action = show_menu(...)` **To**: `key = show_menu(...)`
2. **Add**: `action = options[key][1]`
3. **Add**: `if callable(action):` check
4. **Update**: exit handling to check `key is None or key == "0"`

### Menu Implementation
```python
from dataset_forge.utils.menu import show_menu
from dataset_forge.utils.color import Mocha

def my_menu():
    """Menu implementation with proper error handling."""
    options = {
        1: ("Option 1", function1),
        2: ("Option 2", function2),
        0: ("ðŸšª Exit", None),
    }
    
    while True:
        try:
            key = show_menu("Menu Title", options, Mocha.lavender)
            if key is None or key == "0":
                break
            action = options[key][1]
            if callable(action):
                action()
        except (KeyboardInterrupt, EOFError):
            print_info("\nExiting...")
            break
```

## UI/Color Scheme

### Printing Utilities
```python
from dataset_forge.utils.printing import print_info, print_success, print_warning, print_error
from dataset_forge.utils.color import Mocha

print_success("Operation completed successfully")
print_warning("Warning: Low memory detected")
print_error("Error: File not found")
print_info("Processing images...")
```

## ML Workflow Patterns

### GPU Acceleration
```python
from dataset_forge.utils.gpu_acceleration import gpu_brightness_contrast, gpu_image_analysis

def gpu_enhanced_processing(image: torch.Tensor) -> torch.Tensor:
    """Apply GPU-accelerated image transformations.
    
    Args:
        image: Input image tensor
        
    Returns:
        Processed image tensor
    """
    # GPU-accelerated transformations
    result = gpu_brightness_contrast(image, brightness=1.1, contrast=1.2)
    # GPU-accelerated analysis
    analysis = gpu_image_analysis(result)
    
    return result
```

### Caching for ML Operations
```python
from dataset_forge.utils.cache_utils import in_memory_cache, disk_cache, model_cache

@in_memory_cache(ttl=300)  # 5 minutes for frequently accessed data
def extract_features(image_path: str) -> np.ndarray:
    """Extract features from image with caching.
    
    Args:
        image_path: Path to image file
        
    Returns:
        Feature vector as numpy array
    """
    # Feature extraction logic
    return features

@model_cache(ttl=3600)  # 1 hour for model loading
def load_ml_model(model_path: str) -> Any:
    """Load ML model with caching.
    
    Args:
        model_path: Path to model file
        
    Returns:
        Loaded model
    """
    # Model loading logic
    return model
```

## Critical Development Rules

### Virtual Environment
- **ALWAYS** activate virtual environment: `venv312\Scripts\activate`
- **ALWAYS** test new additions in the correct Python environment

### Documentation
- **ALWAYS** update README.md for new features (with user confirmation)
- **ALWAYS** look up documentation for new packages/modules
- **ALWAYS** maintain comprehensive docstrings

### Error Handling
- **ALWAYS** use centralized error utilities
- **ALWAYS** trigger error sounds for user-facing errors
- **ALWAYS** log operations for debugging

### Performance
- **ALWAYS** use lazy imports for heavy operations
- **ALWAYS** implement proper memory cleanup
- **ALWAYS** use parallel processing for I/O operations
- **ALWAYS** cache expensive computations

### Testing
- **ALWAYS** write comprehensive tests for new features
- **ALWAYS** use pytest (never unittest)
- **ALWAYS** include type annotations in tests
- **ALWAYS** test both success and failure scenarios

### Menu System (MANDATORY)
- **ALWAYS** use the standardized key-based menu pattern
- **NEVER** use `action = show_menu(...); action()` pattern
- **ALWAYS** use key lookup: `action = options[key][1]`
- **ALWAYS** check if action is callable before executing
- **ALWAYS** handle None and "0" exit cases properly

## Project Structure

### Repository Organization
```
DatasetForge/
â”œâ”€â”€ dataset_forge/
â”‚   â”œâ”€â”€ menus/           # UI components
â”‚   â”œâ”€â”€ actions/         # Business logic
â”‚   â”œâ”€â”€ utils/           # Shared utilities
â”‚   â””â”€â”€ dpid/           # Degradation Process Implementation
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_utils/     # Unit tests
â”‚   â””â”€â”€ test_cli/       # Integration tests
â”œâ”€â”€ docs/               # Documentation
â”œâ”€â”€ configs/            # Configuration files
â”œâ”€â”€ tools/              # Development tools
â”œâ”€â”€ assets/             # Static assets
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ setup.py           # Package setup
â”œâ”€â”€ main.py            # Entry point
â””â”€â”€ run.bat            # Windows runner
```

### Test Execution
```bash
# Activate virtual environment
venv312\Scripts\activate

# Run all tests
venv312\Scripts\python -m pytest --maxfail=5 --disable-warnings -v tests/

# Run specific test file
venv312\Scripts\python -m pytest tests/test_utils/test_cache_utils.py -v

# Run with output capture disabled
venv312\Scripts\python -m pytest -s --maxfail=5 --disable-warnings -v tests/
```

## Communication Style

- **Professional and Concise**: Provide clear, actionable guidance
- **Explain Rationale**: When making suggestions, explain the reasoning
- **Consider Trade-offs**: Discuss potential benefits and drawbacks
- **Ask for Clarification**: If requirements are unclear, ask questions
- **Provide Examples**: Include concrete code examples when helpful

## Problem-Solving Approach

1. **Understand the Context**: Analyze the specific Dataset Forge use case
2. **Follow Established Patterns**: Use existing utilities and conventions
3. **Consider Performance**: Optimize for large dataset processing
4. **Focus on User Experience**: Ensure smooth CLI interactions
5. **Test Thoroughly**: Validate functionality and edge cases
6. **Document Changes**: Update relevant documentation

## Style Consistency Best Practices

1. **Don't Refactor Beyond Scope**: Match existing style without broader changes
2. **Comment Adaptation**: Match existing comment style and frequency
3. **Variable Naming**: Use consistent variable naming patterns
4. **Paradigm Alignment**: Favor the dominant paradigm seen in codebase
5. **Library Usage**: Prefer libraries already in use
6. **Gradual Enhancement**: Only introduce newer patterns if already appearing
7. **Organization Mirroring**: Structure new modules like existing ones
8. **Specificity Over Assumptions**: Ask rather than assume if styles inconsistent
9. **Documentation Matching**: Match documentation style in tone and format
10. **Testing Consistency**: Follow established testing patterns

## Adaptation Techniques

1. **Pattern Mirroring**: Copy structural patterns from similar functions
2. **Variable Naming Dictionary**: Create concept-to-name pattern mappings
3. **Comment Density Matching**: Count comments-per-line and match
4. **Error Pattern Replication**: Use identical error handling approaches
5. **Module Structure Cloning**: Organize new modules like existing ones
6. **Import Order Replication**: Order imports using same conventions
7. **Test Case Templating**: Base new tests on existing test structure
8. **Function Size Consistency**: Match function/method granularity
9. **State Management Consistency**: Use same state management approaches
10. **Type Definition Matching**: Format type definitions consistently 

## UI/CLI

### Global Command System (MANDATORY)
- All menus must support global commands:
  - `help`, `h`, `?` â€” Show context-aware help for the current menu (uses centralized help system)
  - `quit`, `exit`, `q` â€” Exit Dataset Forge from any menu, with full cleanup
  - `0` â€” Go back to previous menu
  - `Ctrl+C` â€” Emergency exit with cleanup
- After using `help`, the menu must be redrawn before prompting for input again (never just the prompt alone)
- All help and quit output must use the Catppuccin Mocha color scheme via centralized printing utilities
- No raw print statements for any global command output
- All help screens and prompts must be testable (see below) 

### Menu Pattern Enforcement (MANDATORY)
- **ALL menus MUST use the standardized key-based pattern**
- **show_menu always returns the key, never the function**
- **All menu loops must follow the exact pattern documented above**
- **No exceptions to this pattern are allowed**
- **All new menus must be reviewed for pattern compliance before merging** 