---
description: 
globs: 
alwaysApply: true
---
---
name: PyTorch Data Integration for Dataset Forge"
description: Ensure PyTorch data handling and integration with Dataset Forge patterns
globs: ["dataset_forge/**/*.py"]
alwaysApply: false
priority: 3
---

# PyTorch Data Integration for Dataset Forge

## Core Principles

### Tensor Operations and Memory Management
```python
import torch
from typing import List, Tuple, Optional, Union
from dataset_forge.utils.memory_utils import clear_memory, to_device_safe
from dataset_forge.utils.printing import print_info, print_error

def safe_tensor_operations(tensor: torch.Tensor, device: str = auto) -> torch.Tensor:Perform safe tensor operations with automatic device management.
    
    Args:
        tensor: Input tensor
        device: Target device (auto, cpu, cuda)
        
    Returns:
        Processed tensor on appropriate device
ry:
        # Safe device placement
        if device == "auto":
            device =cuda if torch.cuda.is_available() else "cpu   
        tensor = to_device_safe(tensor, device)
        
        # Perform operations
        result = tensor.float() / 2550  # Normalize to [0, 1]
        
        return result
        
    except RuntimeError as e:
        print_error(fCUDA memory error: {e}")
        # Fallback to CPU
        tensor = tensor.cpu()
        return tensor.float() / 255.0
    except Exception as e:
        print_error(f"Tensor operation error: {e}")
        raise
```

### Image Processing with PyTorch
```python
from PIL import Image
import torchvision.transforms as transforms
from dataset_forge.utils.image_ops import get_image_size

def create_image_transform(target_size: Tuple[int, int], 
                          normalize: bool = True) -> transforms.Compose:Create image transformation pipeline.
    
    Args:
        target_size: Target image size (width, height)
        normalize: Whether to normalize pixel values
        
    Returns:
        Transform pipeline
  ransform_list = [
        transforms.Resize(target_size),
        transforms.ToTensor(),
    ]
    
    if normalize:
        transform_list.append(transforms.Normalize(
            mean=[00.4850.406],
            std=[00.229240.225]
        ))
    
    return transforms.Compose(transform_list)

def process_image_batch(image_paths: List[str], 
                       transform: transforms.Compose,
                       batch_size: int = 32 -> torch.Tensor:
  Process a batch of images with PyTorch.
    
    Args:
        image_paths: List of image file paths
        transform: Image transformation pipeline
        batch_size: Number of images to process at once
        
    Returns:
        Batch tensor of processed images
    esults =
    
    for i in range(0 len(image_paths), batch_size):
        batch_paths = image_paths[i:i + batch_size]
        batch_tensors = []
        
        for path in batch_paths:
            try:
                with Image.open(path) as img:
                    # Convert to RGB if necessary
                    if img.mode != 'RGB':
                        img = img.convert('RGB')
                    
                    # Apply transformations
                    tensor = transform(img)
                    batch_tensors.append(tensor)
                    
            except Exception as e:
                print_error(f"Error processing {path}: {e})          continue
        
        if batch_tensors:
            # Stack tensors into batch
            batch = torch.stack(batch_tensors)
            results.append(batch)
            
            # Clear memory after each batch
            clear_memory()
    
    if results:
        return torch.cat(results, dim=0)
    else:
        return torch.empty(0)
```

### Model Loading and Inference
```python
from dataset_forge.utils.cache_utils import model_cache
from dataset_forge.utils.gpu_acceleration import GPUImageProcessor

@model_cache(ttl=3600# 1 hour cache for model loading
def load_pytorch_model(model_path: str, device: str = "auto)-> torch.nn.Module:Load PyTorch model with caching and device management.
    
    Args:
        model_path: Path to model file
        device: Target device (auto, cpu, cuda)
        
    Returns:
        Loaded model
 ry:
        if device == "auto":
            device =cuda if torch.cuda.is_available() else "cpu        
        # Load model
        model = torch.load(model_path, map_location=device)
        model.eval()
        
        print_info(fModel loaded on {device}")
        return model
        
    except Exception as e:
        print_error(f"Error loading model: {e})        raise

def run_inference(model: torch.nn.Module, 
                 input_tensor: torch.Tensor,
                 batch_size: int = 32 -> torch.Tensor:
Run inference with proper memory management.
    
    Args:
        model: PyTorch model
        input_tensor: Input tensor
        batch_size: Batch size for inference
        
    Returns:
        Model predictions
    ith torch.no_grad():
        results = []
        
        for i in range(0, len(input_tensor), batch_size):
            batch = input_tensor[i:i + batch_size]
            
            # Run inference
            with torch.cuda.amp.autocast() if torch.cuda.is_available() else nullcontext():
                output = model(batch)
                results.append(output)
            
            # Clear memory
            clear_memory()
        
        return torch.cat(results, dim=0)
```

### DataLoader Integration
```python
from torch.utils.data import Dataset, DataLoader
from dataset_forge.utils.progress_utils import tqdm

class ImageDataset(Dataset):
 Custom dataset for image processing.
    
    Args:
        image_paths: List of image file paths
        transform: Image transformation pipeline
    ef __init__(self, image_paths: List[str], transform: transforms.Compose):
        self.image_paths = image_paths
        self.transform = transform
    
    def __len__(self) -> int:
        return len(self.image_paths)
    
    def __getitem__(self, idx: int) -> torch.Tensor:
        path = self.image_paths[idx]
        
        try:
            with Image.open(path) as img:
                if img.mode != 'RGB':
                    img = img.convert('RGB)            return self.transform(img)
        except Exception as e:
            print_error(f"Error loading {path}:[object Object]e}")
            # Return zero tensor as fallback
            return torch.zeros(3, 224, 224

def create_dataloader(image_paths: List[str], 
                     transform: transforms.Compose,
                     batch_size: int = 32,
                     num_workers: int = 4-> DataLoader:
    Create DataLoader with proper configuration.
    
    Args:
        image_paths: List of image file paths
        transform: Image transformation pipeline
        batch_size: Batch size
        num_workers: Number of worker processes
        
    Returns:
        Configured DataLoader
 ataset = ImageDataset(image_paths, transform)
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available(),
        persistent_workers=num_workers > 0
    )
```

## Best Practices

### Memory Management
- **Device Safety**: Use to_device_safe for automatic device placement
- **Batch Processing**: Process data in manageable batches
- **Memory Cleanup**: Clear memory after each batch
- **Gradient Management**: Use torch.no_grad() for inference

### Performance Optimization
- **Mixed Precision**: Use torch.cuda.amp for GPU acceleration
- **DataLoader**: Use proper DataLoader configuration
- **Caching**: Cache model loading and expensive operations
- **Parallel Processing**: Use multiple workers for data loading

### Error Handling
- **Device Fallback**: Graceful fallback to CPU when GPU unavailable
- **Memory Errors**: Handle CUDA out of memory errors
- **File Errors**: Handle corrupted or missing image files
- **Model Errors**: Handle model loading and inference errors

### Integration with Dataset Forge
```python
from dataset_forge.utils.memory_utils import memory_context, auto_cleanup
from dataset_forge.utils.progress_utils import tqdm
from dataset_forge.utils.history_log import log_operation

@auto_cleanup
def pytorch_workflow(image_paths: List[str]) -> List[Dict[str, Any]]:
   Complete PyTorch workflow with Dataset Forge integration.
    
    Args:
        image_paths: List of image file paths
        
    Returns:
        List of processing results
    "ith memory_context("PyTorch Workflow"):
        # Create transform
        transform = create_image_transform((224)
        
        # Process images
        results = []
        for path in tqdm(image_paths, desc=Processing images"):
            try:
                # Load and process image
                with Image.open(path) as img:
                    tensor = transform(img)
                
                # Run inference (example)
                # prediction = model(tensor.unsqueeze(0))
                
                results.append({
                path                tensor_shape': tensor.shape,
                    status': 'success'
                })
                
            except Exception as e:
                results.append({
                path                  error                  status': 'failed'
                })
        
        log_operation("pytorch_workflow", f"Processed {len(image_paths)} images")
        return results
```

## Communication and Problem-Solving

- **Clear Documentation**: Document model architecture and data flow
- **Performance Monitoring**: Track GPU memory usage and inference time
- **User Feedback**: Provide progress updates for long operations
- **Error Recovery**: Suggest solutions for common PyTorch issues

## Code Quality and Best Practices

- **Type Annotations**: Use comprehensive type hints
- **Error Handling**: Robust error handling with meaningful messages
- **Logging**: Comprehensive logging for debugging
- **Documentation**: Clear docstrings and usage examples

## Semantic Naming and Abstractions

- **Descriptive Names**: Use clear, descriptive variable and function names
- **Consistent Patterns**: Follow established naming conventions
- **Abstraction Levels**: Choose appropriate abstraction levels
- **Domain Language**: Use domain-specific terminology

## Platform Thinking

- **Cross-Platform**: Ensure compatibility across different platforms
- **Resource Awareness**: Be aware of GPU memory and system resources
- **Scalability**: Design for scalability and growth
- **Maintainability**: Focus on long-term maintainability

## Response Format

- **Clear Structure**: Organize responses with clear headings and sections
- **Code Examples**: Include practical code examples
- **Explanation**: Explain the reasoning behind PyTorch decisions
- **Trade-offs**: Discuss benefits and potential drawbacks

## Handling Uncertainty and Limitations

- **Graceful Degradation**: Handle cases where GPU is unavailable
- **Fallback Strategies**: Provide fallback options for failed operations
- **User Communication**: Clearly communicate limitations and alternatives
- **Continuous Improvement**: Iterate and improve PyTorch integration strategies
