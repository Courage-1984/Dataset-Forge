---
description: 
globs: 
alwaysApply: true
---
---
name: "Testing Standards for Dataset Forge"
description: "Enforce comprehensive testing patterns using pytest with monkeypatching, dummy objects, and public APIs for all Dataset Forge features"
globs: ["**/*.py", "tests/**/*.py"]
alwaysApply: true
priority: 2
---

# Testing Standards for Dataset Forge

## Core Testing Principles

### Test Framework Requirements
- **ONLY** use pytest or pytest plugins, **NEVER** use the unittest module
- **ALWAYS** place tests in `./tests/` directory with proper structure
- **ALWAYS** create necessary `__init__.py` files if they don't exist
- **ALWAYS** include typing annotations in tests
- **ALWAYS** include docstrings in tests

### Test Organization
- **Unit Tests**: `tests/test_utils/` for utility functions and core logic
- **Integration Tests**: `tests/test_cli/` for end-to-end workflows and menu interactions
- **Test Naming**: Use `test_*.py` files with descriptive names
- **Test Functions**: Use descriptive function names starting with `test_`

## Dataset Forge Specific Testing Patterns

### Public API Requirements
```python
# All features must provide public, non-interactive APIs for testing
def process_images(image_paths: List[str], output_dir: str) -> List[str]:
    """
    Process a list of images and save results to output directory.
    Args:
        image_paths: List of input image file paths
        output_dir: Directory to save processed images
    Returns:
        List of output image file paths
    Raises:
        FileNotFoundError: If input files don't exist
        PermissionError: If output directory is not writable
    """
    # Implementation here
    pass
```

### Monkeypatching and Dummy Objects
```python
import pytest
from unittest.mock import patch, MagicMock, Mock

def test_feature_with_mock():
    """Test feature using monkeypatching to isolate logic."""
    with patch('dataset_forge.utils.audio_utils.play_done_sound') as mock_audio:
        with patch('dataset_forge.utils.memory_utils.clear_memory') as mock_memory:
            # Test implementation
            result = process_images(['test.jpg'], 'output/')
            # Verify calls
            mock_audio.assert_called_once()
            mock_memory.assert_called_once()
            assert result == ['output/test.jpg']
```

### Multiprocessing Test Patterns
```python
# Module-level worker functions for pickling compatibility
def worker_function(data):
    """Worker function for multiprocessing tests."""
    return process_data(data)

def test_multiprocessing_feature():
    """Test multiprocessing features with module-level workers."""
    with patch('dataset_forge.utils.parallel_utils.ProcessingType') as mock_type:
        result = parallel_map(worker_function, [1, 2, 3])
        assert result == [1, 4, 9]
```

### Test Imports and Fixtures
```python
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from _pytest.capture import CaptureFixture
    from _pytest.fixtures import FixtureRequest
    from _pytest.logging import LogCaptureFixture
    from _pytest.monkeypatch import MonkeyPatch
    from pytest_mock.plugin import MockerFixture

import pytest
from unittest.mock import patch, MagicMock

@pytest.fixture
def sample_image_path(tmp_path):
    """Create a sample image for testing."""
    image_path = tmp_path / "test.jpg"
    # Create dummy image
    return str(image_path)
```

## Test Coverage Requirements

### Required Test Coverage
- **Core Business Logic**: All functions in `dataset_forge/actions/`
- **Utility Functions**: All functions in `dataset_forge/utils/`
- **Menu Interactions**: All menu workflows in `dataset_forge/menus/`
- **Integration Flows**: End-to-end workflows and CLI interactions
- **Error Handling**: Both success and failure scenarios
- **Edge Cases**: Boundary conditions and error conditions

### Test Categories
```python
# Unit tests for individual functions
def test_process_single_image():
    """Test processing a single image."""
    pass

# Integration tests for workflows
def test_visual_dedup_workflow():
    """Test complete visual deduplication workflow."""
    pass

# CLI integration tests
def test_menu_interaction():
    """Test menu interaction and user input handling."""
    pass

# Performance tests
def test_memory_management():
    """Test memory cleanup and CUDA operations."""
    pass
```

## Testing Best Practices

### Isolation and Independence
- **Isolate Tests**: Use monkeypatching to avoid external dependencies
- **Dummy Objects**: Create realistic but lightweight test data
- **No External Dependencies**: Avoid reliance on real files, networks, or binaries
- **Cross-Platform**: Ensure tests work on Windows and Linux

### Error Testing
```python
def test_error_handling():
    """Test error handling and recovery."""
    with pytest.raises(FileNotFoundError):
        process_images(['nonexistent.jpg'], 'output/')
    with pytest.raises(PermissionError):
        process_images(['test.jpg'], '/readonly/')
```

### XFAIL and Expected Failures
```python
import pytest
@pytest.mark.xfail(reason="Known limitation with ignore patterns")
def test_directory_tree_ignore_patterns():
    """Test that fails due to known limitation."""
    pass
```

## Test Execution

### Running Tests
```bash
# Activate virtual environment
venv312\Scripts\activate

# Run all tests
venv312\Scripts\python -m pytest --maxfail=5 --disable-warnings -v tests/

# Run specific test file
venv312\Scripts\python -m pytest tests/test_utils/test_cache_utils.py -v

# Run with output capture disabled
venv312\Scripts\python -m pytest -s --maxfail=5 --disable-warnings -v tests/
```

### Test Runner Script
```bash
# Use the flexible test runner
python tools/run_tests.py

# Or specify option directly
python tools/run_tests.py 2  # Recommended mode
```

## Test Maintenance

### Documentation Requirements
- **Test Docstrings**: All test functions must have descriptive docstrings
- **Test Comments**: Explain complex test logic and edge cases
- **Test Documentation**: Document XFAIL tests and their rationale
- **Test Updates**: Update tests when features change

### Code Quality in Tests
- **Type Annotations**: Use type hints in test functions
- **PEP 8 Compliance**: Follow Python style guidelines
- **Import Organization**: Follow the same import patterns as main code
- **Error Messages**: Provide clear, actionable error messages

## Integration with Static Analysis

### Static Analysis Tool
- **Location**: `tools/find_code_issues/find_code_issues.py`
- **Test Coverage**: Checks for untested code and missing test coverage
- **Test/Code Mapping**: Ensures tests exist for all public functions
- **Documentation**: Enforces docstring requirements in tests

### Pre-PR Requirements
- **Run Static Analysis**: Address all actionable issues before submitting PR
- **Test Coverage**: Ensure new features have appropriate test coverage
- **Test Quality**: Verify tests are robust and isolated
- **Documentation**: Update test documentation as needed

## Advanced Testing Patterns

### Memory Management Testing
```python
def test_memory_cleanup():
    """Test memory cleanup after operations."""
    with patch('dataset_forge.utils.memory_utils.clear_memory') as mock_clear:
        with patch('dataset_forge.utils.memory_utils.clear_cuda_cache') as mock_cuda:
            process_large_dataset(['test.jpg'])
            mock_clear.assert_called()
            mock_cuda.assert_called()
```

### Parallel Processing Testing
```python
def test_parallel_processing():
    """Test parallel processing with mocked executors."""
    with patch('concurrent.futures.ThreadPoolExecutor') as mock_executor:
        mock_executor.return_value.__enter__.return_value.map.return_value = [1, 2, 3]
        result = smart_map(lambda x: x * 2, [1, 2, 3])
        assert result == [1, 2, 3]  # Mocked result
```

### Audio and UI Testing
```python
def test_audio_feedback():
    """Test audio feedback integration."""
    with patch('dataset_forge.utils.audio_utils.play_done_sound') as mock_audio:
        with patch('dataset_forge.utils.audio_utils.play_error_sound') as mock_error:
            # Test success case
            process_images(['test.jpg'], 'output/')
            mock_audio.assert_called_once()
            # Test error case
            with pytest.raises(Exception):
                process_images(['nonexistent.jpg'], 'output/')
            mock_error.assert_called_once()
```

## Test Environment Setup

### Virtual Environment
- **ALWAYS** activate virtual environment: `venv312\Scripts\activate`
- **ALWAYS** ensure all dependencies are available in the correct Python environment
- **ALWAYS** test with the same Python version as production

### Test Data Management
- **Temporary Files**: Use `tmp_path` fixture for temporary test files
- **Cleanup**: Ensure proper cleanup of test artifacts
- **Isolation**: Each test should be independent and not affect others
- **Realistic Data**: Use realistic but lightweight test data

## Continuous Integration

### CI Requirements
- **Automated Testing**: Run pytest on every push/PR
- **Cross-Platform**: Test on Windows and Linux
- **Coverage Reporting**: Track test coverage metrics
- **Static Analysis**: Run static analysis in CI pipeline

### Test Reporting
- **Clear Output**: Provide clear, actionable test output
- **Failure Analysis**: Help developers understand test failures
- **Performance Metrics**: Track test execution time and performance
- **Coverage Reports**: Generate and maintain coverage reports



